import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model, TaskType
from src.lora_data_loader import load_data, build_prompt_applier, build_full_prompt_applier
from src.zero_shot_parsers import pyd_parser, pyd_format
import argparse
import itertools

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    ## ä¸è¦å¯ç”¨thinkingï¼Œè¿™é‡Œboolç±»å‹ä¼šæŠ¥é”™
    parser.add_argument("--think", type=bool, default=False)
    parser.add_argument("--lora_r", type=int, default=8)
    parser.add_argument("--lora_alpha", type=int, default=16)
    parser.add_argument("--lora_dropout", type=float, default=0.1)
    parser.add_argument("--load_batch_size", type=int, default=32)
    parser.add_argument("--train_batch_size", type=int, default=16)
    parser.add_argument("--eval_batch_size", type=int, default=16)
    parser.add_argument("--num_train_epochs", type=int, default=3)
    parser.add_argument("--learning_rate", type=float, default=2e-4)
    parser.add_argument("--warmup_steps", type=int, default=100)
    parser.add_argument("--debug", action="store_true", default=False)
    args = parser.parse_args()

    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()

    """load model, dataset, tokenizer, prompt applier"""
    model_name = "Qwen/Qwen3-0.6B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    ## ä¸ä½¿ç”¨device_map, ç»Ÿä¸€äº¤ç»™trainerå¤„ç†
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")
    train_dataset, _, validation_dataset = load_data()

    if args.debug:
        train_dataset = train_dataset.select(range(10))
        validation_dataset = validation_dataset.select(range(10))

    full_prompt_applier = build_full_prompt_applier(tokenizer, enable_thinking=args.think)
    short_prompt_applier = build_prompt_applier(tokenizer, enable_thinking=args.think)



    """è½½å…¥éªŒè¯é›†ï¼Œä½¿ç”¨å®Œæ•´promptï¼ˆåŒ…å«ç­”æ¡ˆï¼‰"""
    # éªŒè¯é›†éœ€è¦å®Œæ•´çš„å¯¹è¯ï¼ˆprompt + answerï¼‰æ¥è®¡ç®—tokenæ‹Ÿåˆåº¦
    validation_full_dataset = validation_dataset.map(
        full_prompt_applier,
        batched=True,
        batch_size=args.load_batch_size
    )
    validation_short_dataset = validation_dataset.map(
        short_prompt_applier,
        batched=True,
        batch_size=args.load_batch_size
    )

    # tokenizeéªŒè¯é›† (ç»“æœæ˜¯python list)
    # ä¸ä½¿ç”¨padding, ç»Ÿä¸€äº¤ç»™data_collatorå¤„ç†
    validation_full_tokenized = validation_full_dataset.map(
        lambda x: tokenizer(x['text'], truncation=True),
        batched=True,
        batch_size=args.load_batch_size
    )
    validation_short_tokenized = validation_short_dataset.map(
        lambda x: tokenizer(x['text'], truncation=True),
        batched=True,
        batch_size=args.load_batch_size
    )

    ## å–æ¶ˆï¼Œç»Ÿä¸€äº¤ç»™trainerå¤„ç†
    # validation_full_tokenized = validation_full_tokenized.remove_columns(["sentence1", "sentence2", "text"])
    # validation_short_tokenized = validation_short_tokenized.remove_columns(["sentence1", "sentence2", "text"])

    # validation_full_tokenized = validation_full_tokenized.rename_column("label", "metric_label")
    # validation_short_tokenized = validation_short_tokenized.rename_column("label", "metric_label")

    # åˆ›å»ºéªŒè¯é›†çš„masked labels (ä½¿ç”¨ python lists)
    validation_masked_labels = []
    for i in range(len(validation_full_tokenized)):
        # ä½¿ç”¨ .copy()
        full_ids = validation_full_tokenized[i]["input_ids"].copy()
        prompt_len = len(validation_short_tokenized[i]["input_ids"])
        
        # ä¸ºåˆ—è¡¨åˆ‡ç‰‡åˆ†é…ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡
        full_ids[:prompt_len] = [-100] * prompt_len

        validation_masked_labels.append(full_ids)

    validation_full_tokenized = validation_full_tokenized.add_column("labels", validation_masked_labels)

    # åœ¨æ‰€æœ‰æ“ä½œå®Œæˆåï¼Œæœ€åè½¬æ¢ä¸ºtorchæ ¼å¼
    # validation_full_tokenized.set_format("torch", columns=["input_ids", "attention_mask", "labels", "metric_label"])
    # å–æ¶ˆï¼Œç»Ÿä¸€äº¤ç»™trainerå¤„ç†



    """prepare train dataset"""
    train_full_dataset = train_dataset.map(
        full_prompt_applier,
        batched=True,
        batch_size=args.load_batch_size
    )

    train_short_dataset = train_dataset.map(
        short_prompt_applier,
        batched=True,
        batch_size=args.load_batch_size
    )

    # ä¸ä½¿ç”¨padding, ç»Ÿä¸€äº¤ç»™data_collatorå¤„ç†
    input_full_tokenized_datasets = train_full_dataset.map(
        lambda x: tokenizer(x['text'], truncation=True),
        batched=True,
        batch_size=args.load_batch_size
    )

    input_short_tokenized_datasets = train_short_dataset.map(
        lambda x: tokenizer(x['text'], truncation=True),
        batched=True,
        batch_size=args.load_batch_size
    )


    """make trianning data for sft, add masked labels"""
    masked_train_labels = []

    for i in range(len(input_full_tokenized_datasets)):
        # ç›´æ¥æ“ä½œ listï¼Œæ— éœ€è½¬æ¢ä¸º tensor å†è½¬å›
        full_ids = input_full_tokenized_datasets[i]["input_ids"]
        prompt_len = len(input_short_tokenized_datasets[i]["input_ids"])
        
        # å…‹éš†ä¸€ä»½ list æ¥åˆ›å»º labels
        labels = list(full_ids)
        labels[:prompt_len] = [-100] * prompt_len
        masked_train_labels.append(labels)

    ## ready to sft
    train_dataset_for_sft = input_full_tokenized_datasets.add_column("labels", masked_train_labels)





    ## æ¸…é™¤æ‰€æœ‰ä¸å¹²å‡€çš„åˆ—
    final_train_dataset = train_dataset_for_sft.remove_columns(
        [col for col in train_dataset_for_sft.column_names if col not in ["input_ids", "labels", "attention_mask"]]
    )
    final_validation_dataset = validation_full_tokenized.remove_columns(
        [col for col in validation_full_tokenized.column_names if col not in ["input_ids", "labels", "attention_mask"]]
    )




    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )

    model = get_peft_model(model, lora_config)

    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        pad_to_multiple_of=8
    )

    training_args = TrainingArguments(
        output_dir="./sft_without_cot_results",
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=2,
        logging_steps=10,
        logging_dir="./logs",
        logging_first_step=True,
        logging_nan_inf_filter=False,
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.train_batch_size,
        per_device_eval_batch_size=args.eval_batch_size,
        num_train_epochs=args.num_train_epochs,
        lr_scheduler_type="cosine",
        warmup_steps=args.warmup_steps,
        log_level="warning",
        report_to=["tensorboard"],
        load_best_model_at_end=True,
        bf16=use_bf16,
        fp16=not use_bf16 and torch.cuda.is_available(),
        dataloader_num_workers=2
    )


    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=final_train_dataset,
        eval_dataset=final_validation_dataset,
        data_collator=data_collator,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )

    if args.debug:
        print("ğŸ” å¼€å§‹è°ƒè¯•...")

        # â€”â€” Begin Debug Block â€”â€”#
        train_loader = trainer.get_train_dataloader()
        batch = next(iter(train_loader))

        input_ids   = batch["input_ids"]      # shape: [B, L]
        attention   = batch["attention_mask"] # shape: [B, L]
        labels      = batch["labels"]         # shape: [B, L]

        for i in range(input_ids.size(0)):
            ids   = input_ids[i].tolist()
            lbls  = labels[i].tolist()
            attn  = attention[i].tolist()

            # è§£ç æ•´æ¡è¾“å…¥ï¼ˆåŒ…å« prompt + answer + padï¼‰
            txt_input = tokenizer.decode(ids, skip_special_tokens=False)

            # æ‰¾åˆ° label != -100 çš„ç´¢å¼•èŒƒå›´
            valid_idxs = [j for j, v in enumerate(lbls) if v != -100]
            if valid_idxs:
                start, end = valid_idxs[0], valid_idxs[-1]
                # è§£ç ç­”æ¡ˆéƒ¨åˆ†
                answer_tokens = lbls[start : end + 1]
                txt_answer = tokenizer.decode(answer_tokens, skip_special_tokens=False)
            else:
                start = end = None
                txt_answer = ""

            print(f"\nâ€”â€” æ ·æœ¬ {i} â€”â€”")
            print("INPUT_DECOD E:", txt_input.replace("\n", "\\n"))
            print("LABEL_TO_PREDICT:", txt_answer.replace("\n", "\\n"))
            print(f"ATTENTION_MASK: {attn}")
            print(f"LABELS_RAW     : {lbls}")

            # è‡ªåŠ¨æ–­è¨€ï¼šprompt æ®µï¼ˆ0..start-1ï¼‰åº”å…¨ä¸º -100
            if start is not None:
                assert all(v == -100 for v in lbls[:start]), \
                    f"Prompt åŒºæ®µæœ‰é -100 å€¼: {lbls[:start]}"
                # ç­”æ¡ˆæ®µï¼ˆstart..endï¼‰åº”å…¨ä¸ä¸º -100
                assert all(v != -100 for v in lbls[start : end+1]), \
                    f"ç­”æ¡ˆåŒºæ®µæœ‰ -100: {lbls[start : end+1]}"
                # pad æ®µï¼ˆend+1..Lï¼‰åº”å…¨ä¸º -100
                assert all(v == -100 for v in lbls[end+1 :]), \
                    f"Pad åŒºæ®µæœ‰é -100 å€¼: {lbls[end+1 :]}"

        print("\nâœ… Debug check passed: prompt/answer/pad åŒºæ®µéƒ½æ­£ç¡®ã€‚")

    # å¼€å§‹è®­ç»ƒï¼ˆæ¯ä¸ªepochä¼šè‡ªåŠ¨è®¡ç®—metricsï¼‰
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()


    # ä¿å­˜æ¨¡å‹
    model.save_pretrained("./sft_without_cot_final")
    tokenizer.save_pretrained("./sft_without_cot_final")

    print("âœ… å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° ./sft_without_cot_final")







