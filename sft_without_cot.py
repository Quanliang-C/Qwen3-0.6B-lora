import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForSeq2Seq
from peft import LoraConfig, get_peft_model, TaskType
from src.lora_data_loader import load_data, build_prompt_applier, build_full_prompt_applier
from src.zero_shot_parsers import pyd_parser, pyd_format

import argparse

parser = argparse.ArgumentParser()
## ä¸è¦å¯ç”¨thinkingï¼Œè¿™é‡Œboolç±»å‹ä¼šæŠ¥é”™
parser.add_argument("--think", type=bool, default=False)
parser.add_argument("--lora_r", type=int, default=8)
parser.add_argument("--lora_alpha", type=int, default=16)
parser.add_argument("--lora_dropout", type=float, default=0.1)
parser.add_argument("--load_batch_size", type=int, default=32)
parser.add_argument("--train_batch_size", type=int, default=16)
parser.add_argument("--eval_batch_size", type=int, default=16)
parser.add_argument("--num_train_epochs", type=int, default=3)
parser.add_argument("--learning_rate", type=float, default=2e-4)
parser.add_argument("--warmup_steps", type=int, default=100)
args = parser.parse_args()

use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()

"""load model, dataset, tokenizer, prompt applier"""
model_name = "Qwen/Qwen3-0.6B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

## ä¸ä½¿ç”¨device_map, ç»Ÿä¸€äº¤ç»™trainerå¤„ç†
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")
train_dataset, _, validation_dataset = load_data()
full_prompt_applier = build_full_prompt_applier(tokenizer, enable_thinking=args.think)
short_prompt_applier = build_prompt_applier(tokenizer, enable_thinking=args.think)



"""è½½å…¥éªŒè¯é›†ï¼Œä½¿ç”¨å®Œæ•´promptï¼ˆåŒ…å«ç­”æ¡ˆï¼‰"""
# éªŒè¯é›†éœ€è¦å®Œæ•´çš„å¯¹è¯ï¼ˆprompt + answerï¼‰æ¥è®¡ç®—tokenæ‹Ÿåˆåº¦
validation_full_dataset = validation_dataset.map(
    full_prompt_applier,
    batched=True,
    batch_size=args.load_batch_size
)
validation_short_dataset = validation_dataset.map(
    short_prompt_applier,
    batched=True,
    batch_size=args.load_batch_size
)

# tokenizeéªŒè¯é›† (ç»“æœæ˜¯python list)
# ä¸ä½¿ç”¨padding, ç»Ÿä¸€äº¤ç»™data_collatorå¤„ç†
validation_full_tokenized = validation_full_dataset.map(
    lambda x: tokenizer(x['text'], truncation=True),
    batched=True,
    batch_size=args.load_batch_size
)
validation_short_tokenized = validation_short_dataset.map(
    lambda x: tokenizer(x['text'], truncation=True),
    batched=True,
    batch_size=args.load_batch_size
)

## å–æ¶ˆï¼Œç»Ÿä¸€äº¤ç»™trainerå¤„ç†
# validation_full_tokenized = validation_full_tokenized.remove_columns(["sentence1", "sentence2", "text"])
# validation_short_tokenized = validation_short_tokenized.remove_columns(["sentence1", "sentence2", "text"])

# validation_full_tokenized = validation_full_tokenized.rename_column("label", "metric_label")
# validation_short_tokenized = validation_short_tokenized.rename_column("label", "metric_label")

# åˆ›å»ºéªŒè¯é›†çš„masked labels (ä½¿ç”¨ python lists)
validation_masked_labels = []
for i in range(len(validation_full_tokenized)):
    # ä½¿ç”¨ .copy()
    full_ids = validation_full_tokenized[i]["input_ids"].copy()
    prompt_len = len(validation_short_tokenized[i]["input_ids"])
    
    # ä¸ºåˆ—è¡¨åˆ‡ç‰‡åˆ†é…ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡
    full_ids[:prompt_len] = [-100] * prompt_len

    validation_masked_labels.append(full_ids)

validation_full_tokenized = validation_full_tokenized.add_column("labels", validation_masked_labels)

# åœ¨æ‰€æœ‰æ“ä½œå®Œæˆåï¼Œæœ€åè½¬æ¢ä¸ºtorchæ ¼å¼
# validation_full_tokenized.set_format("torch", columns=["input_ids", "attention_mask", "labels", "metric_label"])
# å–æ¶ˆï¼Œç»Ÿä¸€äº¤ç»™trainerå¤„ç†



"""prepare train dataset"""
train_full_dataset = train_dataset.map(
    full_prompt_applier,
    batched=True,
    batch_size=args.load_batch_size
)

train_short_dataset = train_dataset.map(
    short_prompt_applier,
    batched=True,
    batch_size=args.load_batch_size
)

# ä¸ä½¿ç”¨padding, ç»Ÿä¸€äº¤ç»™data_collatorå¤„ç†
input_full_tokenized_datasets = train_full_dataset.map(
    lambda x: tokenizer(x['text'], truncation=True),
    batched=True,
    batch_size=args.load_batch_size
)

input_short_tokenized_datasets = train_short_dataset.map(
    lambda x: tokenizer(x['text'], truncation=True),
    batched=True,
    batch_size=args.load_batch_size
)


"""make trianning data for sft, add masked labels"""
masked_train_labels = []

for i in range(len(input_full_tokenized_datasets)):
    # ç›´æ¥æ“ä½œ listï¼Œæ— éœ€è½¬æ¢ä¸º tensor å†è½¬å›
    full_ids = input_full_tokenized_datasets[i]["input_ids"]
    prompt_len = len(input_short_tokenized_datasets[i]["input_ids"])
    
    # å…‹éš†ä¸€ä»½ list æ¥åˆ›å»º labels
    labels = list(full_ids)
    labels[:prompt_len] = [-100] * prompt_len
    masked_train_labels.append(labels)

## ready to sft
train_dataset_for_sft = input_full_tokenized_datasets.add_column("labels", masked_train_labels)





## æ¸…é™¤æ‰€æœ‰ä¸å¹²å‡€çš„åˆ—
final_train_dataset = train_dataset_for_sft.remove_columns(
    [col for col in train_dataset_for_sft.column_names if col not in ["input_ids", "labels", "attention_mask"]]
)
final_validation_dataset = validation_full_tokenized.remove_columns(
    [col for col in validation_full_tokenized.column_names if col not in ["input_ids", "labels", "attention_mask"]]
)




lora_config = LoraConfig(
    r=args.lora_r,
    lora_alpha=args.lora_alpha,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=args.lora_dropout,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)

data_collator = DataCollatorForSeq2Seq(
    model=model,
    tokenizer=tokenizer,
    pad_to_multiple_of=8
)

training_args = TrainingArguments(
    output_dir="./sft_without_cot_results",
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    logging_steps=10,
    logging_dir="./logs",
    logging_first_step=True,
    logging_nan_inf_filter=False,
    learning_rate=args.learning_rate,
    per_device_train_batch_size=args.train_batch_size,
    per_device_eval_batch_size=args.eval_batch_size,
    num_train_epochs=args.num_train_epochs,
    lr_scheduler_type="cosine",
    warmup_steps=args.warmup_steps,
    log_level="warning",
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    bf16=use_bf16,
    fp16=not use_bf16 and torch.cuda.is_available(),
    dataloader_num_workers=2
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=final_train_dataset,
    eval_dataset=final_validation_dataset,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

# å¼€å§‹è®­ç»ƒï¼ˆæ¯ä¸ªepochä¼šè‡ªåŠ¨è®¡ç®—metricsï¼‰
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
trainer.train()

# ä¿å­˜æ¨¡å‹
model.save_pretrained("./sft_without_cot_final")
tokenizer.save_pretrained("./sft_without_cot_final")

print("âœ… å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° ./sft_without_cot_final")







